{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-28T14:31:02.394979Z",
     "start_time": "2024-09-28T14:31:00.578447Z"
    }
   },
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fast_bert.utils import MultiLabelDataset, train_epoch_v2, eval_model_v2\n",
    "from model import MultiTaskRuBERT\n",
    "import joblib\n",
    "\n",
    "# Загрузка тренировочного и валидационного файлов\n",
    "# df = pd.read_excel('02_Реальные_кейсы.xlsx', sheet_name='Реальные кейсы 800 с ответами')\n",
    "\n",
    "train_files = ['train_data_text.xlsx', 'paraphrased_train_all_data_2.xlsx']\n",
    "train_dfs = [pd.read_excel(file) for file in train_files]\n",
    "df_train = pd.concat(train_dfs, ignore_index=True)\n",
    "df_val = pd.read_excel('val_data_text.xlsx')\n",
    "\n",
    "# Фильтрация данных (удаление NaN значений)\n",
    "df_train_filtered = df_train[['Вопрос пользователя', 'Классификатор 1 уровня', 'Классификатор 2 уровня']].dropna()\n",
    "df_val_filtered = df_val[['Вопрос пользователя', 'Классификатор 1 уровня', 'Классификатор 2 уровня']].dropna()\n",
    "\n",
    "# Загрузка обученных LabelEncoder\n",
    "le_lvl1 = joblib.load('le_lvl1.pkl')\n",
    "le_lvl2 = joblib.load('le_lvl2.pkl')\n",
    "\n",
    "# Преобразование текстовых классов в числовые метки\n",
    "df_train_filtered['label_lvl1'] = le_lvl1.transform(df_train_filtered['Классификатор 1 уровня'])\n",
    "df_train_filtered['label_lvl2'] = le_lvl2.transform(df_train_filtered['Классификатор 2 уровня'])\n",
    "df_val_filtered['label_lvl1'] = le_lvl1.transform(df_val_filtered['Классификатор 1 уровня'])\n",
    "df_val_filtered['label_lvl2'] = le_lvl2.transform(df_val_filtered['Классификатор 2 уровня'])\n",
    "\n",
    "# Разделение данных на тексты и метки\n",
    "train_texts = df_train_filtered['Вопрос пользователя'].values\n",
    "train_labels_lvl1 = df_train_filtered['label_lvl1'].values\n",
    "train_labels_lvl2 = df_train_filtered['label_lvl2'].values\n",
    "\n",
    "val_texts = df_val_filtered['Вопрос пользователя'].values\n",
    "val_labels_lvl1 = df_val_filtered['label_lvl1'].values\n",
    "val_labels_lvl2 = df_val_filtered['label_lvl2'].values\n",
    "\n",
    "# Определение количества классов\n",
    "n_classes_lvl1 = len(le_lvl1.classes_)\n",
    "n_classes_lvl2 = len(le_lvl2.classes_)\n",
    "\n",
    "# Пример вывода для проверки\n",
    "print(\"Количество классов 1 уровня:\", n_classes_lvl1)\n",
    "print(\"Количество классов 2 уровня:\", n_classes_lvl2)\n",
    "# print(\"Тренировочные тексты:\", train_texts[:5])\n",
    "# print(\"Валидационные тексты:\", val_texts[:5])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество классов 1 уровня: 13\n",
      "Количество классов 2 уровня: 53\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:32:47.727178Z",
     "start_time": "2024-09-28T14:32:47.036585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Инициализация модели\n",
    "model_name = \"ai-forever/sbert_large_nlu_ru\"\n",
    "model = MultiTaskRuBERT(model_name, n_classes_lvl1, n_classes_lvl2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_len = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "# Создание DataLoader-ов\n",
    "train_dataset = MultiLabelDataset(train_texts, train_labels_lvl1, train_labels_lvl2, tokenizer, max_len)\n",
    "val_dataset = MultiLabelDataset(val_texts, val_labels_lvl1, val_labels_lvl2, tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ],
   "id": "2d8322cc135d83da",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 84%, 80%"
   ],
   "id": "1e020cc017f5c4e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, correct_bias=False)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss(label_smoothing=0.4)\n",
    "loss_fn = FocalLoss(alpha=0.25, gamma=4)\n",
    "best_accuracy_lvl1 = 0\n",
    "best_accuracy_lvl2 = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc_lvl1, train_acc_lvl2, train_loss = train_epoch_v2(\n",
    "        model,\n",
    "        train_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(train_dataset)\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} lvl1 accuracy {train_acc_lvl1} lvl2 accuracy {train_acc_lvl2}')\n",
    "\n",
    "    val_acc_lvl1, val_acc_lvl2, val_loss = eval_model_v2(\n",
    "        model,\n",
    "        val_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(val_dataset)\n",
    "    )\n",
    "\n",
    "    print(f'Validation loss {val_loss} lvl1 accuracy {val_acc_lvl1} lvl2 accuracy {val_acc_lvl2}')\n",
    "\n",
    "    if val_acc_lvl1 > best_accuracy_lvl1 and val_acc_lvl2 > best_accuracy_lvl2 and val_acc_lvl2 > 0.8:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy_lvl1 = val_acc_lvl1\n",
    "        best_accuracy_lvl2 = val_acc_lvl2"
   ],
   "id": "27968e666d651052",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_texts[1]"
   ],
   "id": "5f919f0e893f32c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict(text, model, tokenizer, le_lvl1, le_lvl2, max_len=64):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_lvl1, outputs_lvl2 = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        _, preds_lvl1 = torch.max(outputs_lvl1, dim=1)\n",
    "        _, preds_lvl2 = torch.max(outputs_lvl2, dim=1)\n",
    "\n",
    "    predicted_class_lvl1 = le_lvl1.inverse_transform(preds_lvl1.cpu().numpy())\n",
    "    predicted_class_lvl2 = le_lvl2.inverse_transform(preds_lvl2.cpu().numpy())\n",
    "    \n",
    "    return predicted_class_lvl1, predicted_class_lvl2\n",
    "\n",
    "le_lvl1 = joblib.load('le_lvl1.pkl')\n",
    "le_lvl2 = joblib.load('le_lvl2.pkl')\n",
    "text = 'хочу себе запись трансляции, чтобы подмонтировать потом, как сделать?'\n",
    "predicted_lvl1, predicted_lvl2 = predict(text, model, tokenizer, le_lvl1, le_lvl2, max_len)\n",
    "print(f\"{predicted_lvl1}, {predicted_lvl2}\")"
   ],
   "id": "8d2851da8651fb04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T14:32:53.550282Z",
     "start_time": "2024-09-28T14:32:52.712433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model_state_1.bin\"))\n",
    "model.eval()\n",
    "loss_fn = FocalLoss(alpha=0.25, gamma=4)\n",
    "val_acc_lvl1, val_acc_lvl2, val_loss = eval_model_v2(\n",
    "        model,\n",
    "        val_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(val_dataset)\n",
    ")\n",
    "print(f\"{val_acc_lvl1}, {val_acc_lvl2}\")"
   ],
   "id": "709c99530b4d1082",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8641975308641975, 0.8395061728395061\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "fb3cfd7a099c1d24"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
